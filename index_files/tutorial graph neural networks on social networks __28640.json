{"name": "tutorial graph neural networks on social networks ", "full_name": " h1 Tutorial Graph Neural Networks on Social Networks Using PyTorch h2 1 Set your expectations of this tutorial h2 2 GNN Graph Convolution Neural Network h2 3 Adjacency matrix Degree matrix an example h2 4 Let us add some intelligence learning h2 5 Time to get hands dirty h3 5 1 Required packages h3 5 2 Dataset exploration h3 5 3 Features encoding h3 5 4 Graph construction and visualization h3 5 5 Construct and train our GNN model h3 7 Some resources that helped me to prepare this tutorial ", "stargazers_count": 0, "forks_count": 0, "description": "This product if we closely look at it is generating a new feature vector that captures the feature of the surrounding nodes for each node based on how those surrounding nodes are connected to it. The elements of A indicate whether pairs of nodes are adjacent i. Those things are under three main categories 1. edge_index x self. parameters lr lr 00001 best_accuracy 0. Our graph degree matrix looks like this degree https dev to uploads. csv which contains the edges indices. csv print 5 top nodes labels print target_df. mean mask loss criterion predictions labels loss loss mask loss torch. In contrast to the pixels representation of images the graph can be visualized as a collection of nodes and edges without having any order. com uploads articles 3p9fzpzt7hd0xxm21p19. values node_features torch. Nodes represent developers platform users who have starred at least 10 repositories and edges are mutual follower relationships between them note that word mutual indicates undirected relationship. imshow sparse_feat_matrix_vis 250 cmap Greys sparse https dev to uploads. So if we can learn node features in the latent space we can do many interesting things. graph1 https dev to uploads. connected by edges or not in the graph. zeros for disconnected nodes and ones for the adjacent nodes. com uploads articles 1kmabqemuvw6asoou45d. png Then the latent space feature representation can be calculated by the following note that you may see different ways to solve this problem in the literature but their common goal is to achieve some level of normalization by exploiting the graph structure eq4 https dev to uploads. 5 Construct and train our GNN modelWe start by encoding the whole data by calling encode_data with light False and construct the full graph by calling construct_graph with light False. item print test samples torch. long accuracy mask accuracy accuracy torch. read_csv data musae_git_target. You might have your own way to represent the image as a graph what matters is that you just have the image to graph concept in your mind. zeros 1 max feats 1 for j in range nodes_included temp np. user and ml_target which is 1 if the user is a machine learning community user and 0 otherwise. And here is where we can start doing interesting things. com uploads articles rha625rk7xhh16jmu1cv. Module def __init__ self num_of_feat f super SocialGNN self. Of course as this field is evolving you will find many interesting original GNN models and many bells and whistles for better suiting your problem. com uploads articles atav9s2229mama09f4xa. Now we are sure that our task is a binary classification problem as we only have 2 classes. You can conceptualize the nodes as the graph entities or objects and the edges are any kind of relation that those nodes may have. The particular problem we will be working on is a typical node classification problem. com uploads articles zoqh07c6rpbw0xg3hdsz. com uploads articles k3r2tdjyexhogymoussd. Intro to graph neural networks presentation https www. plot test_losses plt. io en latest notes installation. Here is the function. draw data_nx pos cmap plt. graph signal X which holds the nodes features of the graph. Know how graph datasets which are expected by GNNs look like. com uploads articles 1rp7gummqde0jbxnseal. com uploads articles jtbjvngengfh8s02mcq1. nn import GCNConvimport networkx as nx 5. mean accuracy return accuracy Now we can define our training function in which we will use torch. zero_grad out net data loss masked_loss predictions out labels data. io en latest modules nn. Below we can see that our simple model reached a very decent accuracy on the test set more than 87. We also need json pandas and numpy and some other packages. This graph is directed because the edges may represent asymmetric relationship for instance if the nodes represent social network users and node 1 follows node 2 it does not mean that node 2 follows node 1. python matplotlib inlineimport jsonimport collectionsimport numpy as npimport pandas as pdimport matplotlibimport matplotlib. com uploads articles ntbtrlzhle5fwetir5rz. pythondata_encoded_vis sparse_feat_matrix_vis encode_data light True n 60 plt. hist feat_counts bins 20 plt. It deserve mentioning here that this Data class is very abstract in a sense that you can add any attribute that you think it describes your graph. item train_accuracy val_accuracy test_accuracy best_accuracy val_accuracy plt. Graph classification where we can answer questions like can we predict chemical compounds activity against lung cancer Remember that chemical compounds can be represented as graphs with compound atoms as its nodes and chemical bonds are its edges. com uploads articles w4admdyj8cvgr3191b5w. HandA https dev to uploads. data import Datafrom torch_geometric. html there are different options if you want to install using Pip Wheels please check your syntax to download the compatible version based on your machine and software. The splitting is not straight forward in our case as we have a single giant graph that should be taken all at once there are some other ways to deal with graph segments but let us assume this is the case for now. pythondef train_social net data epochs 10 lr 0. num_nodes 100 print This is a big graph can not plot. The solution of this problem is to normalize the latent features by dividing by the graph degree matrix D specifically we multiply by its inverse. We can one hot encode those features by writing our function encode_data. We will construct our graph object using this class and passing the following attributes noting that all arguments are torch tensors. We need to convert our homogeneous graph to a NetworkX graph then pot using NetworkX. y mask data. plot val_losses plt. read_csv data musae_git_edges. values edges_list edges. I hope this works for you as well note that I put the sign to avoid causing some people to cringe. We have a large social network of GitHub developers named musae github https snap. com uploads articles 3ys8qq5snrztj0hrw1zw. You can take a few seconds to relate the numbers on the calculation to the graph. Stacking those functions together promote this equation to be a deep learning model. array data_encoded str j. step train_losses loss train_accuracy masked_accuracy predictions out labels data. Let us have an example of undirected graphs which has some numerical features attached to its nodes as well as its edges. We will also need NetworkX Python package we will use it to visualize the graph I installed with pip https networkx. title Features distribution plt. I am pasting here a dataset statistics table from the same source so we can have a preliminary idea about what we are dealing with. Time to get hands dirty Even though the above functions look natural and manageable to implement using many machine learning frameworks torch_geometric PyTorch Geometric https pytorch geometric. convert import to_networkxfrom torch_geometric. The second histogram tells that most of the users have between 15 and about 23 features and very few of them have more than 30 and less than 5 features. com uploads articles 4bb0obvymmv1w1k2i8k1. Below we can see how sparse is the matrix that we construct for nodes features. As we agreed we will use torch_geometric. Graph neural networks as their name tells are neural networks that work on graphs. It does this by sliding a kernel a. png At this point I believe you agree with me that we are ready to construct our GNN model class. Particularly we can leverage our last equation to be multiplied by a learnable weight matrix W and apply a nonlinear function to the product as in the next equation. html convolutional layers. We will run the training for some number of epochs and we keep track for the best validation accuracy. Tutorial Graph Neural Networks on Social Networks Using PyTorch 1. png We can express the above graph using two matrices which can capture its structure. Let us add some intelligence learning So far we discussed how we can calculate latent features of a graph data structure. From torch_geometric we need to import AddTrainValTestMask this will help us to segregate the training and test sets later. show hists https dev to uploads. pythondef encode_data light False n 60 if light True nodes_included n elif light False nodes_included len data_raw data_encoded for i in range nodes_included one_hot_feat np. T edge_index02 0 edge_index01 1 edge_index02 1 edge_index01 0 edge_index0 torch. com uploads articles 8eiarf89ehm31tj71m7k. You can see how the nodes are connected by edges and labeled by color. conv2 x edge_index return x As our model will predict the class of all nodes in the graph we however want to calculate the loss and accuracy for specific set based on the phase we are in. The degree matrix is a diagonal matrix that contains information about the number of edges attached to each vertex. round best_accuracy 4 print Epoch Train_Loss. png The second problem is the risk that may result from aggregating the features as we might end up with extremely high values which can lead to computation instability. We may also see how nodes are different based on the number of features. For example we need to have 2 edges between node 100 and node 200 one edge points from 100 to 200 and the other points from 200 to 100. plot train_accuracies plt. pythonclass SocialGNN torch. load json_data edges pd. conv1 GCNConv num_of_feat f self. The dataset referenced paper is Multi scale Attributed Node Embedding https arxiv. We will stack two GCNConv layers the first has input features equal to the number of features in our graph and some arbitrary number of output features f. Of course you are so welcome to post comments if you have any question or you feel that I deviated from what I promised you here 2. But for the sake of this tutorial we will keep it simple and use only one flavor of GNNs which is the graph convolutional operator class GCNConv which is implemented from the paper Semi supervised Classification with Graph Convolutional Networks https arxiv. nodes pos nx. This statistics tell us that our data set has those many nodes and those many edges. You can think of the pixels of the image as the nodes of the graph and the edges are present only between the adjacent pixels. plot test_accuracies plt. Working with data a golden rule is to learn as we go about the dataset. For our graph we come up with a symmetric matrix with zero diagonal as shown below. Data which is a plain old python object to model a single graph with various optional attributes. tensor edges_list dtype torch. format ep 1 epochs loss. CrossEntropyLoss as our loss criterion. This target feature was derived from the job title of each user. conv1 x x edge_index edge_index x F. In the forward function GCNConv can accept many arguments x as the nodes features edge_index and edge_weight in our case we only use the first two arguments. 02907 by Kipf Thomas N. argmax predictions axis 1 labels. tensor node_features_list node_labels torch. So now we can define the modified adjacency matrix \u00c3 as follows abar https dev to uploads. GNN Graph Convolution Neural Network A __BIG__ __caveat__ here is to emphasize that I do not mean GNNs are just CNNs that operate on graphs but what I want to say is that I felt comfortable with GNNs when I linked it to my understanding of CNNs and learnt something about graphs. Our task is to predict whether the GitHub user is a web or a machine learning developer. We will not try to visualize this big graph as I am assuming that you are using your local computer with limited resources. transforms import AddTrainValTestMask as maskingfrom torch_geometric. datset https dev to uploads. The other matrix is the features matrix H a. item split https dev to uploads. If you get this then it is easier for you to think of GNNs as a generic version of CNNs that can operate on graphs. com uploads articles 4auwrxa91qjlb4f37e0o. float mask torch. figure figsize 12 8 nx. tensor target_df ml_target. cat edge_index01 edge_index02 axis 1 g Data x node_features y node_labels edge_index edge_index0 g_light Data x node_features 0 2 y node_labels edge_index edge_index0 55 if light return g_light else return g For drawing a graph we construct our draw_graph function. This dot product leads to aggregate the neighboring pixel values to one representative scaler. Node classification our tutorial problem falls under this category as we are given a graph of a social network where users represent its nodes and the relationships between them are its edges and our task is to predict each user group i. num_node_featuresnet SocialGNN num_of_feat num_of_feat f 16 criterion nn. figure figsize 25 25 plt. Most of us if not all already know that images consist of pixels and the CNN leverages the convolution operation to calculate latent hidden features for different pixels based on their surrounding pixel values. functional as Ffrom torch_geometric. title Number of features per graph distribution plt. It also tells us that the edges has no features and no directions. 4 Graph construction and visualizationTo construct our graph we will use torch_geometric. Construct and train a simple GNN model for node classification task based on convolutional GNN using torch_geometric the geometric deep learning extension library for PyTorch. pyplot as pltimport torchimport torch. Now we will build construct_graph function which does the following pythondef construct_graph data_encoded light False node_features_list list data_encoded. Yes convolutional neural networks the deep neural network class that is most commonly applied to image analysis and computer vision applications. ml_target bins 4 plt. 3 Features encodingThe nodes features tell us which feature is attached to each node. GCNConv class however there are many other layers you can try on PyTorch Geometric documentation https pytorch geometric. com uploads articles o74ml5tspubldlcttkkb. Then we apply a relu activation function and deliver the latent features to the second layer which has output nodes equal to the number our classes i. org documentation stable install. pythonnum_of_feat g. You may have different split ratios but in this way we may have a more realistic performance and we will not overfit easily I know you might disagree with me on this point We can also print the graph information and the number of nodes each set mask. com watch v 8owQBFAHw7E by Petar Veli\u010dkovi\u0107 on YouTube. But for our tutorial let us make some visually appealing illustrations. round val_accuracy 4 np. com uploads articles 7yxbpebsz6oqjf7fv3lo. We can learn the optimal W based on the task at hand. png hdash1 https dev to uploads. We also plot the losses and accuracies across epochs for the training. plot train_losses plt. html a geometric deep learning extension library for PyTorch provides many variations of deep learning on graphs and other irregular structures. float mask mask torch. val_mask val_losses val_loss val_accuracy masked_accuracy predictions out labels data. val_mask val_accuracies val_accuracy test_accuracy masked_accuracy predictions out labels data. The first is that when calculating the entry of the features matrix H for any of the nodes we exclude that particular node features from the calculation as shown by red zeros. by edge features as in our case or can be unweighted i. 4f Train_Accuracy. Thank you for following this tutorial I hope it was of help for you. show At this point we constructed all the required functions and ready to instantiate our model and train it. Now let us twist our conceptualization of images a little bit and think of images as a graphs. All this information can give us a feeling that our dataset is a right candidate for this tutorial in term of simplicity. eq1 https dev to uploads. We can see already signs of that in our simple example on the latent feature matrix H which has entries at least 10 times more than the entries in the original H. com uploads articles ebkho4uzy0a30jzahmbp. The tensor shape will be 2 2 number_of_original_edges. conv2 GCNConv f 2 def forward self data x data. only 60 nodes for the purpose of visualization. For sure there are many other variations of GNNs but let us stick to this for those 10 minutes of reading. png Implementation wise W can be a linear transformation e. json which contains the nodes features. We will define functions masked_loss and masked_accuracy for which we pass the respective masks and it returns the corresponding loss and accuracy. We read them and plot the top 5 rows and the last 5 rows from the labels file. For instance during training we want to calculate the training loss and accuracy only based on the training set and here where we should use our masks. png hdash2 https dev to uploads. This is a way to represent the undirected graph if we are given the edge indecies. pythondef draw_graph data0 node_labels data0. CrossEntropyLoss train_social net g epochs 50 lr 0. item print validation samples torch. Also you should know that both nodes and edges can have features a. 0 train_losses train_accuracies val_losses val_accuracies test_losses test_accuracies for ep in range epochs 1 optimizer. hist feats bins 50 plt. I am aiming at the end of this step by step tutorial that you will be able to 1. Those elements can be weighted e. pythondata_encoded _ encode_data light False g construct_graph data_encoded data_encoded light False The usual immediate step after data preparation in the machine learning pipelines is to perform data segregation and split subsets of the dataset to train the model and further validate how it performs against new data and save a test segment to report the overall performance. 2 Dataset explorationI assume the dataset files are in a subfolder named data. Adjacency matrix Degree matrix an example Let us take a simple type of graphs the undirected graph this graph type is simple because the relationship between nodes is symmetric. The benefit of using GNNs is the provision of a generalized form that enables us to exploit non Euclidean space data. Our plan is to use this function to encode a light subset of the graph e. backward optimizer. For instance we can add a metadata attribute g meta_data bla bla bla which make it flexible to encapsulate any information you would like. png Another important property we need to look at the class balance this is because severe class imbalances may cause the classifier to simply guessing the majority class and not making any evaluation on the underrepresented class. spring_layout data_nx scale 1 plt. 01 optimizer torch. y will be assigned to the node labels its shape is number_of_nodes edge_index to represent an undirected graph we need to extend the original edge indices in a way that we can have two separate directed edges connecting the same two nodes but pointing opposite to each other. to_markdown headandtail https dev to uploads. This leads to some issues but let us say for now it is not a severe problem. pythonmsk masking split train_rest num_splits 1 num_val 0. 1 epochs https dev to uploads. I do not know if now you got the same intuition of mine which is true that we can stack as many functions of those as we like and learn the weight matrices W using backpropagation. relu x x self. The figure below represents a graph that consists of 4 nodes represented by those blue circles with indices 1 2 3 and 4 and connected by directed edges represented by those green arrows. return else data_nx to_networkx data0 node_colors data0. We will construct the model with 16 filters and will use nn. x will be assigned to the encoded node features its shape is number_of_nodes number_of_features. The idea is to calculate the loss and accuracy for all nodes and multiply it by the mask to zero out the not required nodes. 4f Test_Accuracy. __init__ self. The first is called the adjacency matrix and mostly denoted by A which is a square matrix used to represent finite graphs like this one. png Since you are now reading this post about GNNs I can confidently assume that you already have knowledge about CNNs. 4f Val_Accuracy. vertices which are connected by the second ingredient edges. png Now if we take a critical look at the result we can observe two problems. 02907 if you would like to have a look at how it was developed. 1 Required packagesTo begin with we must download and import the required packages. pythonwith open data musae_git_features. Then we can pass it to draw_graph to show the following graph. AddTrainValTestMask class can take our graph and let us set how we want our masks to formed and it will add a node level split via train_mask val_mask and test_mask attributes. reshape 1 1 sparse_feat_matrix np. 6 g msk g print g print print training samples torch. But if we want to accomplish a particular task we can guide this calculation toward our goal. 4f. test_mask test_accuracies test_accuracy if np. attributes note the can as having features is not a must. json as json_data data_raw json. And the graph is a data structure that has two main ingredients nodes a. pythong_sample construct_graph data_encoded data_encoded_vis light True draw_graph g_sample graphlish https dev to uploads. to_markdown print print 5 last nodes print target_df. concatenate sparse_feat_matrix temp axis 0 sparse_feat_matrix sparse_feat_matrix 1 return data_encoded sparse_feat_matrix elif light False return data_encoded None We can plot the first 250 features columns the total is 4005 of the code for the first 60 users. Those features can represent any attributes that describe a node blue boxes or an edge green boxes. Gain insights about what graph neural networks GNNs are and what type of problems they may solve. Then our graph has the streets as its edges and their intersections as its nodes. png After downloading the dataset we can see 3 important files musae_git_edges. html which was collected from the public API. array 0 max feats 1 this_feat data_raw str i one_hot_feat this_feat 1 data_encoded str i list one_hot_feat if light True sparse_feat_matrix np. The node features are extracted based on the location repositories starred employer and e mail address. filter over the input image and calculating the dot product between that small filter and the overlapping image area. get_cmap Set1 node_color node_colors node_size 600 connectionstyle angle3 width 1 with_labels False edge_color k arrowstyle Now we can draw a sub graph by calling construct_graph with light True. csv which contains the targets i. graph0 https dev to uploads. edu data github social. png An interesting operation that we may like to do is to calculate the dot product between those two matrices. mean mask accuracy torch. train_mask loss. We will download and explore a social network dataset collected from GitHub. In order to tell the training phase which nodes that should be included during training and tell the inference phase which are the test data we can use masks which are binary vectors that indicate using 0 and 1 which nodes belong to each particular mask. The third histogram shows the most common features accross users we can see different peaks on the distribution. Even though we see 4 columns but only 2 columns concern us here id of the node i. Construct graphs and visualize them using code. The numbers between the brackets is the shape of each attribute tensor. Set your expectations of this tutorialYou can follow this tutorial if you would like to know about Graph Neural Networks GNNs through a practical example using PyTorch framework. Some resources that helped me to prepare this tutorial 1. gnneq1 https dev to uploads. In our training we use 30 as a validation set and 60 as test set while we keep only 10 for the training. Semi supervised Classification with Graph Convolutional Networks Paper https arxiv. a linear layer of the product and \u03a6 is a non linear activation function. mean loss return loss def masked_accuracy predictions labels mask mask mask. float edge_index data. By plotting the histogram frequency distribution we see some imbalance as the machine learning label 1 are fewer than the other classes. tolist edge_index01 torch. Edge classification as an example we can predict the congestion in a particular street if we are given the city map. Also the data has no temporal property. train_mask train_accuracies train_accuracy val_loss masked_loss predictions out labels data. T edge_index02 torch. PyTorch Geometric Documentation https pytorch geometric. Mathematicians like to be concise and they represent the graph as the set G V E where G V Eare the graph vertices and edges sets respectively. Features are normally vectors but we represent them by scalers here for simplicity. The contrary is the undirected graph which has bidirectional edges then the relation could be friendship for example. plot val_accuracies plt. title Classes distribution plt. pythondef masked_loss predictions labels mask mask mask. Which seems to be absurd in the sense that we are ignoring our central node To solve this we can add self connections to every node on the graph and this can be achieved by adding the identity matrix I to the graph adjacency matrix which will alter its zero valued diagonal to ones. To download torch_geometric please follow strictly the installation guide here https pytorch geometric. ", "id": "awadelrahman/tutorial-graph-neural-networks-on-social-networks", "size": "28640", "language": "python", "html_url": "https://www.kaggle.com/code/awadelrahman/tutorial-graph-neural-networks-on-social-networks", "git_url": "https://www.kaggle.com/code/awadelrahman/tutorial-graph-neural-networks-on-social-networks", "script": "", "entities": "(('how it', 'look'), '02907') (('we', 'edge indecies'), 'be') (('i', 'True sparse_feat_matrix np'), 'feat') (('graph degree matrix', 'uploads'), 'look') (('Now us', 'graphs'), 'let') (('also how nodes', 'features'), 'see') (('we', 'training'), 'use') (('which', 'structure'), 'express') (('train_losses val_losses val_accuracies', 'range epochs 1 optimizer'), '0') (('python matplotlib inlineimport jsonimport collectionsimport', 'matplotlibimport pdimport matplotlib'), 'numpy') (('to_markdown', 'uploads'), 'headandtail') (('pythondef masked_loss predictions', 'mask mask mask'), 'label') (('we', '100'), 'need') (('which', 'equal number'), 'apply') (('we', 'two problems'), 'png') (('that', 'node blue boxes'), 'represent') (('which', 'various optional attributes'), 'datum') (('k Now we', 'light True'), 'node_size') (('classifier', 'underrepresented class'), 'png') (('nodes', 'particular mask'), 'in') (('Even above functions', 'machine learning many frameworks'), 'time') (('idea', 'required nodes'), 'be') (('total', 'first 60 users'), 'axi') (('you', 'limited resources'), 'try') (('we', 'draw_graph function'), 'edge_index01') (('we', 'what'), 'paste') (('how we', 'graph data structure'), 'discuss') (('mean loss return loss masked_accuracy def predictions', 'mask mask mask'), 'label') (('feature', 'node'), 'feature') (('relationship', 'nodes'), 'matrix') (('we', 'opposite other'), 'assign') (('GCNConv however many other you', 'PyTorch Geometric documentation https pytorch geometric'), 'class') (('node', '2 node'), 'direct') (('which', 'public API'), 'html') (('this', 'training sets'), 'need') (('you', 'mind'), 'have') (('which', 'computation instability'), 'be') (('dataset', 'simplicity'), 'give') (('5 last nodes', 'target_df'), 'to_markdown') (('you', '1'), 'aim') (('common goal', 'eq4 https uploads'), 'calculate') (('train_mask train_accuracies val_loss masked_loss', 'labels data'), 'train_accuracy') (('we', 'GNN model class'), 'png') (('how nodes', 'color'), 'see') (('weight matrices', 'backpropagation'), 'know') (('We', 'then NetworkX.'), 'need') (('that', 'tutorial'), 'resource') (('pairs', 'nodes'), 'indicate') (('nodes bonds', 'compound atoms'), 'classification') (('we', 'torch_geometric'), 'construct') (('us', 'reading'), 'be') (('that', 'green arrows'), 'represent') (('we', 'two matrices'), 'png') (('We', 'labels last 5 file'), 'read') (('we', 'required packages'), 'begin') (('G V graph where vertices', 'G V set E'), 'like') (('I', 'you'), 'be') (('we', 'torch'), 'mean') (('construct_graph', 'data_encoded light False node_features_list list'), 'build') (('it', 'issues'), 'lead') (('simple model', 'more than 87'), 'see') (('item val_accuracy', 'best_accuracy val_accuracy plt'), 'train_accuracy') (('having', 'features'), 'note') (('We', 'training'), 'plot') (('we', 'dataset'), 'be') (('We', 'nodes'), 'have') (('neural that', 'graphs'), 'be') (('Then graph', 'nodes'), 'have') (('node features', 'starred employer'), 'extract') (('We', 'function'), 'can') (('5 Construct', 'light False'), 'start') (('input', 'output features arbitrary f.'), 'have') (('how surrounding nodes', 'it'), 'generate') (('this', 'graph all once other segments'), 'be') (('specifically we', 'inverse'), 'be') (('I', 'pip https networkx'), 'need') (('numbers', 'attribute tensor'), 'be') (('We', 'GitHub developers'), 'have') (('people', 'sign'), 'hope') (('it', 'corresponding loss'), 'define') (('val_mask masked_accuracy', 'labels data'), 'val_accuracies') (('We', 'GitHub'), 'download') (('we', 'torch_geometric'), 'use') (('step train_losses train_accuracy masked_accuracy', 'labels data'), 'loss') (('here where we', 'interesting things'), 'be') (('nodes', 'that'), 'conceptualize') (('task', 'user group i.'), 'classification') (('learning extension geometric deep library', 'graphs'), 'provide') (('we', 'many interesting things'), 'do') (('Even we', 'columns node only 2 here i.'), 'see') (('dot product', 'one representative scaler'), 'lead') (('it', 'graph'), 'deserve') (('some', 'visually appealing illustrations'), 'let') (('we', 'goal'), 'want') (('data set', 'many nodes'), 'tell') (('We', 'hand'), 'learn') (('nodes', 'only adjacent pixels'), 'think') (('dataset files', 'subfolder'), 'assume') (('last equation', 'next equation'), 'leverage') (('we', 'distribution'), 'show') (('we', 'city map'), 'predict') (('normally we', 'here simplicity'), 'be') (('we', 'validation best accuracy'), 'run') (('particular node', 'red zeros'), 'be') (('we', 'it'), 'show') (('nodes_included len False data_raw', 'range nodes_included one_hot_feat np'), 'light') (('We', 'nn'), 'construct') (('that', 'graphs'), 'be') (('which', 'original H.'), 'see') (('which', 'GNNs'), 'know') (('graph', 'order'), 'visualize') (('Then we', 'following graph'), 'pass') (('data that', 'a.'), 'be') (('we', 'files 3 important musae_git_edges'), 'png') (('edges', 'features'), 'tell') (('we', 'zero diagonal'), 'come') (('plan', 'graph e.'), 'be') (('Semi', 'Graph Convolutional Networks Paper https arxiv'), 'supervise') (('nodes', 'graph'), 'signal') (('which', 'ingredient second edges'), 'vertex') (('together equation', 'functions'), 'promote') (('things', 'three main categories'), 'be') (('it', 'train_mask val_mask'), 'take') (('target feature', 'user'), 'derive') (('You', 'graph'), 'take') (('which', 'ones'), 'add') (('nodes', 'features'), 'see') (('confidently you', 'CNNs'), 'assume') (('it', 'you'), 'thank') (('diagonal that', 'vertex'), 'be') (('html different you', 'machine'), 'be') (('which', 'one'), 'call') (('then relation', 'example'), 'be') (('you', 'information'), 'add') (('nodes', 'a.'), 'know') (('network convolutional neural deep neural that', 'image most commonly analysis'), 'network') (('csv 5 top nodes', 'print'), 'print') (('arguments', 'following attributes'), 'construct') (('We', 'json also pandas'), 'need') (('word mutual', 'undirected relationship'), 'note') (('they', 'problems'), 'solve') (('we', 'phase'), 'conv2') (('further how it', 'overall performance'), 'pythondata_encoded') (('mask loss criterion predictions', 'loss loss mask loss torch'), 'mean') (('which', 'nodes'), 'let') (('classification binary we', 'only 2 classes'), 'be') (('most', 'more than 30 less than 5 features'), 'tell') (('that', 'space non Euclidean data'), 'be') (('\u03a6', 'linear product'), 'layer') (('CNN', 'pixel surrounding values'), 'know') (('Semi', 'Graph Convolutional Networks https arxiv'), 'keep') (('when I', 'graphs'), 'Network') (('we', 'only first two arguments'), 'accept') (('So now we', 'uploads'), 'define') (('GNN many interesting original models', 'many better problem'), 'evolve') (('you', 'PyTorch framework'), 'follow') (('machine learning label', '1 other classes'), 'see') (('where we', 'masks'), 'want') "}